{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM/fNiSrswC0288qWTe/WTN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MengOonLee/WebScrapy/blob/master/Groceries/Lotus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install --no-cache-dir -qU scrapy selenium"
      ],
      "metadata": {
        "id": "I3g1lntAeoPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile Lotus.py\n",
        "import time\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.common.by import By\n",
        "import scrapy\n",
        "from scrapy.crawler import CrawlerProcess\n",
        "\n",
        "class LotusSpider(scrapy.Spider):\n",
        "    name = 'Lotus'\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        options = webdriver.chrome.options.Options()\n",
        "        options.add_argument(\"--headless\")\n",
        "        options.add_argument(\"--no-sandbox\")\n",
        "        options.add_argument(\"--enable-javascript\")\n",
        "        options.add_argument(\"--enable-cookies\")\n",
        "        options.add_argument(\"--disable-notifications\")\n",
        "        options.add_argument(\"--disable-web-security\")\n",
        "        options.add_argument(\"--incognito\")\n",
        "        self.driver = webdriver.Chrome(options=options)\n",
        "\n",
        "    def start_requests(self):\n",
        "        urls = [\n",
        "            \"https://www.lotuss.com.my/en/product/aus-mutton-loin-slice-5318-74539337\"\n",
        "        ]\n",
        "        for url in urls:\n",
        "            yield scrapy.Request(url=url, callback=self.parse_items)\n",
        "\n",
        "    # def parse_categories(self, response):\n",
        "    #     self.driver.get(response.url)\n",
        "\n",
        "    #     elem_present = \"\"\n",
        "    #     while not elem_present:\n",
        "    #         try:\n",
        "    #             elem_present = self.driver.find_element(By.XPATH,\n",
        "    #                 \"//div[@class='product-grid-item']\"\n",
        "    #             )\n",
        "    #         except:\n",
        "    #             continue\n",
        "\n",
        "    #     selector = scrapy.Selector(text=self.driver.page_source)\n",
        "    #     category = selector.css(\"h1#category-title::text\").get()\n",
        "    #     print(category)\n",
        "    #     category_links = selector.css(\"div.carousel a\")\n",
        "    #     if len(category_links)!=0:\n",
        "    #         yield from response.follow_all(category_links,\n",
        "    #             callback=self.parse_categories)\n",
        "    #     else:\n",
        "    #         last_height = self.driver.execute_script(\n",
        "    #             \"return document.body.scrollHeight\"\n",
        "    #         )\n",
        "\n",
        "    #         while True:\n",
        "    #             self.driver.execute_script(\n",
        "    #                 \"window.scrollTo(0, document.body.scrollHeight)\"\n",
        "    #             )\n",
        "    #             time.sleep(3)\n",
        "    #             new_height = self.driver.execute_script(\n",
        "    #                 \"return document.body.scrollHeight\"\n",
        "    #             )\n",
        "    #             if new_height==last_height:\n",
        "    #                 break\n",
        "    #             last_height = new_height\n",
        "\n",
        "    #         selector = scrapy.Selector(text=self.driver.page_source)\n",
        "    #         item_links = selector.css(\"div#product-list a\")\n",
        "\n",
        "    #         yield from response.follow_all(item_links,\n",
        "    #             callback=self.parse_items)\n",
        "\n",
        "    def parse_items(self, response):\n",
        "        print(response.url)\n",
        "        # self.driver.get(response.url)\n",
        "\n",
        "    #     elem_present = \"\"\n",
        "    #     while not elem_present:\n",
        "    #         try:\n",
        "    #             elem_present = self.driver.find_element(By.XPATH, \"\"\"\n",
        "    #                 //img[@id='current-product-image']\n",
        "    #             \"\"\")\n",
        "    #         except:\n",
        "    #             continue\n",
        "\n",
        "    #     selector = scrapy.Selector(text=self.driver.page_source)\n",
        "    #     item = selector.css(\"div.MuiBox-root\")\n",
        "    #     name = item.css(\"h1::text\").get()\n",
        "    #     price = item.css(\"\"\"\n",
        "    #         span::text\n",
        "    #     \"\"\").getall()\n",
        "    #     print(name, price)\n",
        "\n",
        "process = CrawlerProcess()\n",
        "process.crawl(LotusSpider)\n",
        "process.start()"
      ],
      "metadata": {
        "id": "LpckwD6FYG2Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64a06c78-4b98-4689-ce2a-e256dedbab64"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing Lotus.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "python Lotus.py"
      ],
      "metadata": {
        "id": "J_TorfYlmX5Y",
        "outputId": "1d142499-88e5-4c79-bac6-68aaa56df378",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://www.lotuss.com.my/en/product/aus-mutton-loin-slice-5318-74539337\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2024-08-17 04:21:45 [scrapy.utils.log] INFO: Scrapy 2.11.2 started (bot: scrapybot)\n",
            "2024-08-17 04:21:45 [scrapy.utils.log] INFO: Versions: lxml 4.9.4.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.9.1, w3lib 2.2.1, Twisted 24.7.0, Python 3.10.12 (main, Jul 29 2024, 16:56:48) [GCC 11.4.0], pyOpenSSL 24.2.1 (OpenSSL 3.2.2 4 Jun 2024), cryptography 42.0.8, Platform Linux-6.1.85+-x86_64-with-glibc2.35\n",
            "2024-08-17 04:21:45 [selenium.webdriver.common.selenium_manager] DEBUG: Selenium Manager binary found at: /usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/linux/selenium-manager\n",
            "2024-08-17 04:21:45 [selenium.webdriver.common.selenium_manager] DEBUG: Executing process: /usr/local/lib/python3.10/dist-packages/selenium/webdriver/common/linux/selenium-manager --browser chrome --language-binding python --output json\n",
            "2024-08-17 04:21:56 [selenium.webdriver.common.selenium_manager] DEBUG: Driver path: /root/.cache/selenium/chromedriver/linux64/127.0.6533.119/chromedriver\n",
            "2024-08-17 04:21:56 [selenium.webdriver.common.selenium_manager] DEBUG: Browser path: /root/.cache/selenium/chrome/linux64/127.0.6533.119/chrome\n",
            "2024-08-17 04:21:56 [selenium.webdriver.common.service] DEBUG: Started executable: `/root/.cache/selenium/chromedriver/linux64/127.0.6533.119/chromedriver` in a child process with pid: 2537 using 0 to output -3\n",
            "2024-08-17 04:21:56 [selenium.webdriver.remote.remote_connection] DEBUG: POST http://localhost:54887/session {'capabilities': {'firstMatch': [{}], 'alwaysMatch': {'browserName': 'chrome', 'pageLoadStrategy': <PageLoadStrategy.normal: 'normal'>, 'browserVersion': None, 'goog:chromeOptions': {'extensions': [], 'binary': '/root/.cache/selenium/chrome/linux64/127.0.6533.119/chrome', 'args': ['--headless', '--no-sandbox', '--enable-javascript', '--enable-cookies', '--disable-notifications', '--disable-web-security', '--incognito']}}}}\n",
            "2024-08-17 04:21:56 [urllib3.connectionpool] DEBUG: Starting new HTTP connection (1): localhost:54887\n",
            "2024-08-17 04:21:56 [urllib3.connectionpool] DEBUG: http://localhost:54887 \"POST /session HTTP/1.1\" 200 0\n",
            "2024-08-17 04:21:56 [selenium.webdriver.remote.remote_connection] DEBUG: Remote response: status=200 | data={\"value\":{\"capabilities\":{\"acceptInsecureCerts\":false,\"browserName\":\"chrome-headless-shell\",\"browserVersion\":\"127.0.6533.119\",\"chrome\":{\"chromedriverVersion\":\"127.0.6533.119 (bdef6783a05f0b3f885591e7d2c7b2aec1a89dea-refs/branch-heads/6533@{#1999})\",\"userDataDir\":\"/tmp/.org.chromium.Chromium.I14hPv\"},\"fedcm:accounts\":true,\"goog:chromeOptions\":{\"debuggerAddress\":\"localhost:39011\"},\"networkConnectionEnabled\":false,\"pageLoadStrategy\":\"normal\",\"platformName\":\"linux\",\"proxy\":{},\"setWindowRect\":true,\"strictFileInteractability\":false,\"timeouts\":{\"implicit\":0,\"pageLoad\":300000,\"script\":30000},\"unhandledPromptBehavior\":\"dismiss and notify\",\"webauthn:extension:credBlob\":true,\"webauthn:extension:largeBlob\":true,\"webauthn:extension:minPinLength\":true,\"webauthn:extension:prf\":true,\"webauthn:virtualAuthenticators\":true},\"sessionId\":\"21fb91ec3fe883c1a088257d90b9d7cb\"}} | headers=HTTPHeaderDict({'Content-Length': '865', 'Content-Type': 'application/json; charset=utf-8', 'cache-control': 'no-cache'})\n",
            "2024-08-17 04:21:56 [selenium.webdriver.remote.remote_connection] DEBUG: Finished Request\n",
            "2024-08-17 04:21:56 [scrapy.addons] INFO: Enabled addons:\n",
            "[]\n",
            "/usr/local/lib/python3.10/dist-packages/scrapy/utils/request.py:254: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
            "\n",
            "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
            "\n",
            "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
            "  return cls(crawler)\n",
            "2024-08-17 04:21:56 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
            "2024-08-17 04:21:56 [scrapy.extensions.telnet] INFO: Telnet Password: 1b3e773120d72f90\n",
            "2024-08-17 04:21:56 [scrapy.middleware] INFO: Enabled extensions:\n",
            "['scrapy.extensions.corestats.CoreStats',\n",
            " 'scrapy.extensions.telnet.TelnetConsole',\n",
            " 'scrapy.extensions.memusage.MemoryUsage',\n",
            " 'scrapy.extensions.logstats.LogStats']\n",
            "2024-08-17 04:21:56 [scrapy.crawler] INFO: Overridden settings:\n",
            "{}\n",
            "2024-08-17 04:21:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
            "['scrapy.downloadermiddlewares.offsite.OffsiteMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
            " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
            " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
            " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
            " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
            " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
            " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
            " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
            " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
            "2024-08-17 04:21:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
            "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
            " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
            " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
            " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
            "2024-08-17 04:21:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
            "[]\n",
            "2024-08-17 04:21:57 [scrapy.core.engine] INFO: Spider opened\n",
            "2024-08-17 04:21:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
            "2024-08-17 04:21:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
            "2024-08-17 04:21:58 [urllib3.connectionpool] DEBUG: Starting new HTTPS connection (1): publicsuffix.org:443\n",
            "2024-08-17 04:21:58 [urllib3.connectionpool] DEBUG: https://publicsuffix.org:443 \"GET /list/public_suffix_list.dat HTTP/1.1\" 200 86865\n",
            "2024-08-17 04:21:58 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.lotuss.com.my/en/product/aus-mutton-loin-slice-5318-74539337> (referer: None)\n",
            "2024-08-17 04:21:58 [scrapy.core.engine] INFO: Closing spider (finished)\n",
            "2024-08-17 04:21:58 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
            "{'downloader/request_bytes': 264,\n",
            " 'downloader/request_count': 1,\n",
            " 'downloader/request_method_count/GET': 1,\n",
            " 'downloader/response_bytes': 2859,\n",
            " 'downloader/response_count': 1,\n",
            " 'downloader/response_status_count/200': 1,\n",
            " 'elapsed_time_seconds': 1.832949,\n",
            " 'finish_reason': 'finished',\n",
            " 'finish_time': datetime.datetime(2024, 8, 17, 4, 21, 58, 969101, tzinfo=datetime.timezone.utc),\n",
            " 'httpcompression/response_bytes': 7752,\n",
            " 'httpcompression/response_count': 1,\n",
            " 'log_count/DEBUG': 4,\n",
            " 'log_count/INFO': 10,\n",
            " 'memusage/max': 68218880,\n",
            " 'memusage/startup': 68218880,\n",
            " 'response_received_count': 1,\n",
            " 'scheduler/dequeued': 1,\n",
            " 'scheduler/dequeued/memory': 1,\n",
            " 'scheduler/enqueued': 1,\n",
            " 'scheduler/enqueued/memory': 1,\n",
            " 'start_time': datetime.datetime(2024, 8, 17, 4, 21, 57, 136152, tzinfo=datetime.timezone.utc)}\n",
            "2024-08-17 04:21:58 [scrapy.core.engine] INFO: Spider closed (finished)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NahwGJoQztOT"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}