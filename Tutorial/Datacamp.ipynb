{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79af9122-4634-46f7-8bd9-b13b8e088653",
   "metadata": {},
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b511ac40-5c08-4a98-88e9-71cacabccb12",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Introduction to HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00485a85-51e2-4b36-be6a-fd118d633bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <tag attrib=\"info\">\n",
    "#  ..contents..\n",
    "# </tag>\n",
    "\n",
    "f = open('test.html', 'w')\n",
    "\n",
    "html = '''\n",
    "<html>\n",
    "  <head>\n",
    "    <title>Website Title</title>\n",
    "    <link rel=\"stylesheet\" type=\"text/css\" href=\"style.css\">\n",
    "  </head>\n",
    "  <body>\n",
    "    <div class=\"class1\" id=\"div1\">\n",
    "      <p class=\"class2\">\n",
    "        Visit <a href=\"http://datacamp.com/\">DataCamp</a>!\n",
    "      </p>\n",
    "    </div>\n",
    "    <div class=\"class1 class3\" id=\"div2\">\n",
    "      <p class=\"class2\">\n",
    "        Or search for it on <a href=\"http://www.google.com\">Google</a>!\n",
    "      </p>\n",
    "    </div>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "f.write(html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef001a8e-cb52-49c7-b503-d929af5d155a",
   "metadata": {},
   "source": [
    "## XPaths & Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0316a99-1041-492f-96cc-033786d47e07",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### XPaths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "638f46da-fea5-4536-b661-dfa046825a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# / = look forward one generation\n",
    "# [] = specific elements\n",
    "xpath = '/html/body/div[2]/p'\n",
    "# // = look forward all future generations\n",
    "xpath = '//p'\n",
    "# @ = attribute\n",
    "xpath = '//span[@class=\"span-class\"]'\n",
    "\n",
    "# Create an XPath string to direct to children of body element\n",
    "# * = wildcard\n",
    "xpath = '/html/body/*'\n",
    "\n",
    "# Create an XPath string to the desired paragraph element\n",
    "xpath = '/html/body/div/div/p'\n",
    "\n",
    "# Create an Xpath string to select desired p element\n",
    "xpath = '//*[@id=\"div3\"]/p'\n",
    "\n",
    "# Create an XPath string to select p element by class\n",
    "xpath = '//p[@class=\"class-1 class-2\"]'\n",
    "\n",
    "# Create an xpath to the href attribute\n",
    "xpath = '//p[@id=\"p2\"]/a/@href'\n",
    "\n",
    "# Create an xpath to the href attributes\n",
    "# contains(@attrib, \"string-expr\")\n",
    "xpath = '//*[contains(@class, \"class-1\")]'\n",
    "xpath = '//a[contains(@class,\"package-snippet\")]/@href'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3c266b-6aab-4ed8-8b0f-a5e304be4d16",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Selectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931381e-c00a-4ce6-a588-45dd76fb9f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div class=\"hello datacamp\">\n",
    "      <p>Hello World!</p>\n",
    "    </div>\n",
    "    <p>Enjoy DataCamp!</p>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "# Create a Selector selecting html as the HTML document\n",
    "sel = scrapy.Selector(text=html)\n",
    "\n",
    "print(sel.xpath(\"//p\").extract())\n",
    "print(sel.xpath(\"//p\").extract_first())\n",
    "print(sel.xpath(\"//p\")[1].extract())\n",
    "\n",
    "# Create a SelectorList of all div elements in the HTML document\n",
    "divs = sel.xpath('//div')\n",
    "print(divs)\n",
    "\n",
    "# Chain together xpath methods to select desired p element\n",
    "sel.xpath('//div').xpath('./p[1]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac12436-3c3a-4901-8b40-02ca32a84a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import a scrapy Selector\n",
    "import scrapy\n",
    "# Import requests\n",
    "import requests\n",
    "\n",
    "# Create the string html containing the HTML source\n",
    "url = 'https://en.wikipedia.org/wiki/Web_scraping'\n",
    "html = requests.get(url).content\n",
    "# Create the Selector object sel from html\n",
    "sel = scrapy.Selector(text=html)\n",
    "# Print out the number of elements in the HTML document\n",
    "print(\"There are 1020 elements in the HTML document.\")\n",
    "print(\"You have found: \", len(sel.xpath('//*')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "565008c4-62a2-4bad-82ac-b65320c9874c",
   "metadata": {},
   "source": [
    "## CSS Locators & Responses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342cc661-ab95-4f83-bc4a-443921457c9e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### CSS Locators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4a40e-217d-4e1c-b23d-0fbd5b0defee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# xpath: /html/body/div          css: html > body > div\n",
    "# xpath: //div/span//p           css: div > span p\n",
    "# xpath: //div/p[2]              css: div > p:nth-of-type(2)\n",
    "# xpath: /html/body//div/p[2]    css: html > body div > p:nth-of-type(2)\n",
    "\n",
    "# Select paragraph elements within class class1\n",
    "css = 'div#uid > p.class1'\n",
    "# Select all elements whose class attribute belongs to class1\n",
    "css = '.class1'\n",
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "css = '#uid > *'\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator\n",
    "xpath = '/html/body/span[1]//a'\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css = 'html > body > span:nth-of-type(1) a'\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator\n",
    "xpath = '//div[@id=\"uid\"]/span//h4'\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css = 'div#uid > span h4'\n",
    "\n",
    "# Create the XPath string equivalent to the CSS Locator\n",
    "xpath = '//div[@id=\"uid\"]/a/@href'\n",
    "# Create the CSS Locator string equivalent to the XPath\n",
    "css = 'div#uid > a::attr(href)'\n",
    "\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]//text()'\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css = 'p#p3 ::text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de268acf-ab29-4944-ba2a-cc11cae61239",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "html = '''\n",
    "<html>\n",
    "  <body>\n",
    "    <div class=\"hello datacamp\">\n",
    "      <p>Hello World!</p>\n",
    "    </div>\n",
    "    <p>Enjoy DataCamp!</p>\n",
    "    <p id=\"p-example\">\n",
    "      Hello world!\n",
    "      Try <a href=\"http://www.datacamp.com\">Datacamp</a> today!\n",
    "    </p>\n",
    "  </body>\n",
    "</html>\n",
    "'''\n",
    "\n",
    "# Create a selector from the html (of a secret website)\n",
    "sel = scrapy.Selector(text=html)\n",
    "\n",
    "print(sel.css('div > p'))\n",
    "print(sel.css('div > p').extract())\n",
    "print(sel.css('p#p-example > a').extract())\n",
    "\n",
    "print(sel.xpath('//p[@id=\"p-example\"]/text()').extract())\n",
    "print(sel.css('p#p-example::text').extract())\n",
    "print(sel.xpath('//p[@id=\"p-example\"]//text()').extract())\n",
    "print(sel.css('p#p-example ::text').extract())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13ea61d6-ca8c-4637-9306-333e93cd5d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scrapy\n",
    "\n",
    "# Create a selector object from a secret website\n",
    "sel = Selector(text=html)\n",
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css('div.course-block > a')\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb75cf5a-7dbd-4568-b06b-ad6c896377e2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c300d989-121e-4eb7-b961-f8e0a6329ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "response.xpath('//div/span[@class=\"bio\"]').extract()\n",
    "response.xpath('div > span.bio').extract_first()\n",
    "response.xpath('//div').css('span.bio').extract()\n",
    "\n",
    "# Get the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title of the website loaded in response\n",
    "this_title = response.xpath('/html/head/title')\\\n",
    "    .css('::text').extract_first()\n",
    "\n",
    "# next_url is the string path of the next url\n",
    "response.follow(next_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9efb4b-891b-4491-a669-66e6fe026e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a CSS Locator string to the desired hyperlink elements\n",
    "css_locator = 'a.course-block__link'\n",
    "\n",
    "# Select the hyperlink elements from response and sel\n",
    "response_as = response.css(css_locator)\n",
    "sel_as = sel.css(css_locator)\n",
    "\n",
    "# Examine similarity\n",
    "nr = len(response_as)\n",
    "ns = len(sel_as)\n",
    "for i in range(min(nr, ns, 2)):\n",
    "    print(\"Element %d from response: %s\" % (i+1, response_as[i]))\n",
    "    print(\"Element %d from sel: %s\" % (i+1, sel_as[i]))\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07402355-6aab-4297-882f-c0bd38b3436d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all desired div elements\n",
    "divs = response.css('div.course-block')\n",
    "\n",
    "# Take the first div element\n",
    "first_div = divs[0]\n",
    "\n",
    "# Extract the text from the (only) h4 element in first_div\n",
    "h4_text = first_div.css('h4::text').extract_first()\n",
    "\n",
    "# Print out the text\n",
    "print(\"The text from the h4 element is:\", h4_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f5b638-2e4f-408d-8aff-7428a8fb0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Response loaded with HTML from https://www.datacamp.com/courses/all\n",
    "course_divs = response.css('div.course-block')\n",
    "print(len(course_divs))\n",
    "\n",
    "# Inspecting course-block\n",
    "first_div = course_divs[0]\n",
    "children = first_div.xpath('./*')\n",
    "print(len(children))\n",
    "\n",
    "# First child\n",
    "first_child = children[0]\n",
    "print(first_child.extract())\n",
    "\n",
    "# CSS Locator\n",
    "links = response.css('div.course-block > a::attr(href)').extract()\n",
    "\n",
    "# Stepwise\n",
    "# Step 1: course blocks\n",
    "course_divs = response.css('div.course-block')\n",
    "# Step 2: hyperlink elements\n",
    "hrefs = course_divs.xpath('./a/@href')\n",
    "# Step 3: extract the links\n",
    "links = hrefs.extract()\n",
    "\n",
    "for l in links:\n",
    "    print(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a919f05a-6d4d-4c6c-a864-4b6321f1a862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a SelectorList of the course titles\n",
    "crs_title_els = response.css('h4::text')\n",
    "\n",
    "# Extract the course titles \n",
    "crs_titles = crs_title_els.extract()\n",
    "\n",
    "# Print out the course titles \n",
    "for el in crs_titles:\n",
    "    print(\">>\", el)\n",
    "\n",
    "# Calculate the number of children of the mystery element\n",
    "how_many_kids = len(mystery.xpath('./*'))\n",
    "\n",
    "# Print out the number\n",
    "print(\"The number of elements you selected was:\", how_many_kids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4544e6-d89f-40ad-9308-4ea0d4288272",
   "metadata": {},
   "source": [
    "## Spiders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50091f76-ff18-4ac4-86b6-2979bc921efd",
   "metadata": {},
   "source": [
    "### Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da64c2e-a27b-4736-8975-77654b06c6dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the spider class\n",
    "class SpiderClass(scrapy.Spider):\n",
    "    name = \"spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        self.print_msg(\"Hello World!\")\n",
    "        urls = ['https://www.datacamp.com']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    # parse method\n",
    "    def parse(self, response):\n",
    "        # simple example: write out the html\n",
    "        html_file = 'DC_courses.html'\n",
    "        with open(html_file, 'wb') as fout:\n",
    "            fout.write(response.body)\n",
    "    # print_msg method\n",
    "    def print_msg(self, msg):\n",
    "        print(\"Calling start_requests in SpiderClass prints out:\", msg)\n",
    "\n",
    "# Initiate a CrawlerProcess\n",
    "process = CrawlerProcess()\n",
    "# Tell the process which spider to use\n",
    "process.crawl(SpiderClass)\n",
    "# Start the crawling process\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "055fac8a-e42a-4567-a7c4-21fb47839a6a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the spider class\n",
    "class SpiderClass(scrapy.Spider):\n",
    "    name = \"spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        urls = ['https://www.datacamp.com/courses/all']\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "    # parse method\n",
    "    def parse(self, response):\n",
    "        links = response.css('div.course-block > a::attr(href)').extract()\n",
    "        for link in links:\n",
    "            yield response.follow(url=link, callback=self.parse2)\n",
    "\n",
    "    def parse2(self, response):\n",
    "        filepath = 'DC_links.csv'\n",
    "        with open(filepath, 'w') as f:\n",
    "            f.writelines(response + '/n')\n",
    "\n",
    "process = CrawlerProcess()\n",
    "process.crawl(SpiderClass)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a407996-7c5f-4246-80c5-20c884edf94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DCspider(scrapy.Spider):\n",
    "    name = 'dcspider'\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url=url_short, callback=self.parse)\n",
    "    # parse method\n",
    "    def parse(self, response):\n",
    "        # Create an extracted list of course author names\n",
    "        author_names = response.css('p.course-block__author-name::text').extract()\n",
    "        # Here we will just return the list of Authors\n",
    "        return author_names\n",
    "\n",
    "process = CrawlwerProcess()\n",
    "process.crawl(DCspider)\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd45417-fe8b-4f0b-a1ea-aa66c9658f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the scrapy library\n",
    "import scrapy\n",
    "\n",
    "# Create the Spider class\n",
    "class DCdescr(scrapy.Spider):\n",
    "    name = 'dcdescr'\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url=url_short, callback=self.parse)\n",
    "    \n",
    "    # First parse method\n",
    "    def parse(self, response):\n",
    "        links = response.css('div.course-block > a::attr(href)').extract()\n",
    "        # Follow each of the extracted links\n",
    "        for link in links:\n",
    "            yield response.follow(url=link, callback=self.parse_descr)\n",
    "      \n",
    "    # Second parsing method\n",
    "    def parse_descr(self, response):\n",
    "        # Extract course description\n",
    "        course_descr = response.css('p.course__description::text').extract_first()\n",
    "        # For now, just yield the course description\n",
    "        yield course_descr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94ae5b4a-c09b-44c8-8856-e00a0b61d7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy library\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        url = 'https://www.datacamp.com/courses/all'\n",
    "            yield scrapy.Request(url=url, callback=self.parse_front)\n",
    "    # parse the front courses page\n",
    "    def parse_front(self, response):\n",
    "        # Narrow in on the course blocks\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        # Direct to the course links\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        # Extract the links (as a list of strings)\n",
    "        links_to_follow = course_links.extract()\n",
    "        # Follow the links to the next parser\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url=url, callback=self.parse_pages)\n",
    "    # parse course pages\n",
    "    def parse_pages(self, response):\n",
    "        # Direct to the course title text\n",
    "        crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "        # Extract and clean the course title text\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        # Direct to the chapter titles text\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        # Extract and clean the chapter titles text\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        # Store this in our dictionary\n",
    "        dc_dict[crs_title_ext] = ch_titles_ext\n",
    "\n",
    "dc_dict = dict()\n",
    "\n",
    "# Initiate a CrawlerProcess\n",
    "process = CrawlerProcess()\n",
    "# Tell the process which spider to use\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "# Start the crawling process\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e735d8fa-d8f6-48ed-ad9d-33feca2ff6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Chapter_Spider(scrapy.Spider):\n",
    "    name = \"dc_chapter_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url=url_short, callback=self.parse_front)\n",
    "    # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url=url, callback=self.parse_pages)\n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        ch_titles = response.css('h4.chapter__title::text')\n",
    "        ch_titles_ext = [t.strip() for t in ch_titles.extract()]\n",
    "        dc_dict[crs_title_ext] = ch_titles_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Chapter_Spider)\n",
    "# Run the Spider\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48b7b644-bf5f-4d6e-bb59-3c1bfdd61784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class DC_Description_Spider(scrapy.Spider):\n",
    "    name = \"dc_description_spider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url=url_short, callback=self.parse_front)\n",
    "    # First parsing method\n",
    "    def parse_front(self, response):\n",
    "        course_blocks = response.css('div.course-block')\n",
    "        course_links = course_blocks.xpath('./a/@href')\n",
    "        links_to_follow = course_links.extract()\n",
    "        for url in links_to_follow:\n",
    "            yield response.follow(url=url, callback=self.parse_pages)\n",
    "    # Second parsing method\n",
    "    def parse_pages(self, response):\n",
    "        # Create a SelectorList of the course titles text\n",
    "        crs_title = response.xpath('//h1[contains(@class, \"title\")]/text()')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_title_ext = crs_title.extract_first().strip()\n",
    "        # Create a SelectorList of course descriptions text\n",
    "        crs_descr = response.css('p.course__description::text')\n",
    "        # Extract the text and strip it clean\n",
    "        crs_descr_ext = crs_descr.extract_first().strip()\n",
    "        # Fill in the dictionary\n",
    "        dc_dict[crs_title_ext] = crs_descr_ext\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(DC_Description_Spider)\n",
    "# Run the Spider\n",
    "process.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2c2f52-2220-4a09-9641-5c7ab66609a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scrapy\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "# Create the Spider class\n",
    "class YourSpider(scrapy.Spider):\n",
    "    name = \"yourspider\"\n",
    "    # start_requests method\n",
    "    def start_requests(self):\n",
    "        yield scrapy.Request(url=url_short, callback=self.parse)\n",
    "    # parse method\n",
    "    def parse(self, response):\n",
    "        # Extracted course titles\n",
    "        crs_titles = response.xpath('//h4[contains(@class, \"block__title\")]/text()')\\\n",
    "            .extract()\n",
    "        # Extracted course descriptions\n",
    "        crs_descrs = response.xpath('//p[contains(@class, \"block__description\")]/text()')\\\n",
    "            .extract()\n",
    "        # Fill in the dictionary: it is the spider output\n",
    "        for crs_title, crs_descr in zip(crs_titles, crs_descrs):\n",
    "            dc_dict[crs_title] = crs_descr\n",
    "\n",
    "# Initialize the dictionary **outside** of the Spider class\n",
    "dc_dict = dict()\n",
    "\n",
    "# Import the CrawlerProcess: for running the spider\n",
    "process = CrawlerProcess()\n",
    "process.crawl(YourSpider)\n",
    "# Run the Spider\n",
    "process.start()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
