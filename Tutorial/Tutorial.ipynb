{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Web_scraping/blob/master/Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./venv.sh\n",
    "pip install --no-cache-dir -U pip wheel build\n",
    "pip install --no-cache-dir -U scrapy\n",
    "pip check\n",
    "rm -rf dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile pip_install.sh\n",
    "#!/bin/bash\n",
    "pip install --no-cache-dir -U pip wheel\n",
    "pip install --no-cache-dir -U numpy pandas matplotlib seaborn\n",
    "pip install --no-cache-dir -U scrapy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XPaths & Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "`<tag attrib=\"attrib info\">...</tag>`\n",
    "  - `<div id=\"unique\" class=\"non unique\">...</div>`  \n",
    "  - `<a href=\"https://...\">...</a>`\n",
    "\n",
    "`@`: attributes\n",
    "  - `@id`, `@class`, `@href`\n",
    "  \n",
    "### XPaths notation\n",
    "\n",
    "xpath = '//*[@id=\"uid\"]/p[2]'  \n",
    "- `/`: look forward one generation  \n",
    "- `[]`: narrow on specific elements  \n",
    "- `//`: look forward all generations  \n",
    "- `*`: wildcard\n",
    "\n",
    "xpath = '//*[contains(@class, \"expr\")]'\n",
    "\n",
    "xpath = '//*/@class'\n",
    "\n",
    "XPath: `<xpath-to-element>/@attr-name`  \n",
    "xpath = '//div[@id=\"uid\"]/a/@href'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selector object\n",
    "# Import a scrapy Selector\n",
    "from scrapy import Selector\n",
    "\n",
    "# Import requests\n",
    "import requests\n",
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get(url).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Outputs the SelectorList:\n",
    "sel.xpath('//p')\n",
    "# out: [<Selector xpath='//p' data='<p>..</p>'>, ...]\n",
    "\n",
    "sel.xpath('//p').extract()\n",
    "# out: ['<p>...</p>', ...]\n",
    "\n",
    "sel.xpath('//p').extract_first()\n",
    "# out: '<p>...</p>'\n",
    "\n",
    "# Text extraction for future generations\n",
    "sel.xpath('//p[@id=\"uid\"]//text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS & Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Locator\n",
    "\n",
    "CSS: Cascading Style Sheets  \n",
    "`/` replaced by `>`\n",
    "- XPath: `/html/body/div`  \n",
    "- CSS: `html > body > div`\n",
    "\n",
    "`//` replaced by ` `\n",
    "- XPath: `//div/span//p`  \n",
    "- CSS: `div > span p`\n",
    "\n",
    "`[N]` replaced by `:nth-of-type(N)`\n",
    "- XPath: `//div/p[2]`  \n",
    "- CSS: `div > p:nth-of-type(2)`\n",
    "\n",
    "`<tag>.<class>`: find element by class  \n",
    "`<tag>#<id>`: find element by id\n",
    "\n",
    "CSS Locator: `<css-to-element>::attr(attr-name)`  \n",
    "css_locator = 'div#uid > a::attr(href)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector from the html\n",
    "sel = Selector(text=html)\n",
    "\n",
    "sel.css('div > p')\n",
    "# out: [<Selector xpath='...' data='<p>...</p>'>, ...]\n",
    "\n",
    "sel.css('div > p').extract()\n",
    "# out: [<p>...</p>, ...]\n",
    "\n",
    "# hyperlink children of all div belongs to class course-block\n",
    "sel.css('div.course-block > a')\n",
    "\n",
    "# all element's class = class-1\n",
    "sel.css('.class-1')\n",
    "\n",
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "sel.css('#uid > *')\n",
    "\n",
    "# Text extraction for future generations\n",
    "sel.css('p#uid ::text').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XPaths Notation & CSS Locators\n",
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector object from a secret website\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css('div.course-block > a')\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href')\n",
    "\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]/text()'\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3::text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response\n",
    "\n",
    "- has all the tools with Selectors  \n",
    "- keeps track of the url  \n",
    "- move from one side to another  \n",
    "\n",
    "XPath:  \n",
    "response.xpath('//div/span[@class=\"bio\"]')  \n",
    "CSS:  \n",
    "response.css('div > span.bio')  \n",
    "Chaining:  \n",
    "response.xpath('//div').css('span.bio')  \n",
    "\n",
    "`response.url`: keeps track URL  \n",
    "`response.follow(next_url)`: follow a new link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title of the website loaded in response\n",
    "this_title = response.xpath('/html/head/title')\\\n",
    "    .css('::text').extract_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['5 Reasons to Attend Extract Summit 2022', '5 Reasons to Attend Extract Summit 2022', 'How to use Playwright with Zyte Smart Proxy Manager', 'How to use Selenium with Zyte Smart Proxy Manager', 'How to use Puppeteer with Zyte Smart Proxy Manager', 'How to avoid web scraping blocks and bans', 'How web scraping is utilized for used car data extraction', 'Scraping large e-commerce websites: a guide for large scale scraping', 'A developer’s guide to rotating proxies in Python\\xa0', 'The importance of web scraping in data journalism', 'How can the travel industry benefit from data scraping?']\n",
      "['/blog/page/2/']\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "url = \"https://www.zyte.com/blog/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "title = sel.css('.oxy-post-title')\\\n",
    "    .css('::text').extract()\n",
    "print(title)\n",
    "\n",
    "next_page = sel.css('a.next')\\\n",
    "    .xpath('./@href').extract()\n",
    "print(next_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./myspider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./myspider.py\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name ='blogspider'\n",
    "    start_urls = [\"https://www.zyte.com/blog/\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for title in response.css('.oxy-post-title'):\n",
    "            yield {'title': title.css('::text').get()}\n",
    "            \n",
    "        for next_page in response.css('a.next'):\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy runspider myspider.py -o myspider.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/tag/humor/page/2/']\n",
      "['Jane Austen', 'Steve Martin', 'Garrison Keillor', 'Jim Henson', 'Charles M. Schulz', 'Suzanne Collins', 'Charles Bukowski', 'Terry Pratchett', 'Dr. Seuss', 'George Carlin']\n",
      "['“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', '“A day without sunshine is like, you know, night.”', '“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”', '“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”', \"“All you need is love. But a little chocolate now and then doesn't hurt.”\", \"“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\", '“Some people never go crazy. What truly horrible lives they must lead.”', '“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”', '“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”', '“The reason I talk to myself is because I’m the only one whose answers I accept.”']\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "url = \"https://quotes.toscrape.com/tag/humor/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "next_page = sel.css('li.next a::attr(\"href\")').extract()\n",
    "print(next_page)\n",
    "\n",
    "quote = sel.css('div.quote')\n",
    "author = quote.xpath('span/small/text()').extract()\n",
    "print(author)\n",
    "text = quote.css('span.text::text').extract()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDDBjF019o71",
    "outputId": "b879accb-647b-42ef-e5fb-f16c2310f99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/tag/humor/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'author': quote.xpath('span/small/text()').get(),\n",
    "                'text': quote.css('span.text::text').get()\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTGIArVO9tCh",
    "outputId": "0a78d94f-3bdb-45b3-d8a5-b575bf8e484c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy runspider quotes_spider.py -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9AL5k0C9wOu",
    "tags": []
   },
   "source": [
    "### Create tutorial project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BjPrLOnu9x3G",
    "outputId": "c60a02c9-98d2-4531-a37f-6981ab18dcc5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./start_project.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./start_project.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIeH3gbpFNwQ",
    "outputId": "12c4a9ef-1052-4731-8090-7d0c27c8705c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tutorial/tutorial/spiders/quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f'quotes-{page}.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ck1_F6jFD53",
    "outputId": "655e1795-902a-4246-fc43-8218aebd00d6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tutorial/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector xpath='descendant-or-self::title/text()' data='Quotes to Scrape'>]\n",
      "['Quotes', 'Scrape']\n",
      "[<Selector xpath='//title/text()' data='Quotes to Scrape'>]\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "url = \"https://quotes.toscrape.com/page/1/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "# CSS\n",
    "title = sel.css('title::text')\n",
    "print(title)\n",
    "print(title.re(r'(\\w+) to (\\w+)'))\n",
    "\n",
    "# XPath\n",
    "title = sel.xpath('//title/text()')\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Er9G3K7RVCwc",
    "outputId": "18047f74-436d-443e-d955-369df8c0421e",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "scrapy shell \"https://quotes.toscrape.com/page/1/\"\n",
    "response.css('title::text').getall()\n",
    "response.css('title::text').get()\n",
    "response.css('title::text').re(r'Quotes.*')\n",
    "response.css('title::text').re(r'Q\\w+')\n",
    "response.css('title::text').re(r'(\\w+) to (\\w+)')\n",
    "response.xpath('//title/text()').getall()\n",
    "response.xpath('//title/text()').get()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eBNvI5rsYNi5",
    "outputId": "582366eb-b4a1-4865-c106-992fc89da345",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "scrapy shell \"https://quotes.toscrape.com\"\n",
    "quote = response.css(\"div.quote\")[0]\n",
    "quote.css(\"span.text::text\").get()\n",
    "quote.css(\"small.author::text\").get()\n",
    "quote.css(\"div.tags a.tag::text\").getall()\n",
    "\n",
    "for quote in response.css(\"div.quote\"):\n",
    "    text = quote.css(\"span.text::text\").get()\n",
    "    author = quote.css(\"small.author::text\").get()\n",
    "    tags = quote.css(\"div.tags a.tag::text\").getall()\n",
    "    print(dict(text=text, author=author, tags=tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting data in spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SnvXeOJt3KK",
    "outputId": "90a06139-1b97-45a6-fefc-4a1271211cf7"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "      'https://quotes.toscrape.com/page/1/',\n",
    "      'https://quotes.toscrape.com/page/2/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBmQ1PfP6SDC",
    "outputId": "35af10e9-bec4-4e9f-d85b-ab4f689819d5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roz26Udn6cDj",
    "outputId": "efd1e2ec-fdc1-43a0-f770-ba15cf448dc3",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.json\n",
    "scrapy crawl quotes -O quotes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok8vKftv7dDq",
    "outputId": "964b3726-9cf6-4ce4-c96e-51323207b78d",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "scrapy shell \"https://quotes.toscrape.com\"\n",
    "response.css(\"li.next a\").get()\n",
    "response.css(\"li.next a::attr(href)\").get()\n",
    "response.css(\"li.next a\").attrib[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "      'https://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supports relative URLs directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        for a in response.css('ul.pager li.next a'):\n",
    "            yield response.follow(a, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multiple requests from an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        anchors = response.css('ul.pager li.next a')\n",
    "        yield from response.follow_all(anchors, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        yield from response.follow_all(css='ul.pager li.next a', callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/author_spider.py\n",
    "import scrapy\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "    \n",
    "    start_urls = ['https://quotes.toscrape.com/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        author_page_links = response.css('.author + a')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJeDMq173KlrdlGG8ohN66",
   "include_colab_link": true,
   "name": "Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
