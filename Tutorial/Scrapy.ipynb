{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Web_scraping/blob/master/Tutorial/Scrapy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XvXGXei0CJFI"
   },
   "source": [
    "# Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vi9PaP1PCJFN"
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --no-cache-dir -qU scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZUWOvXRPCJFT",
    "outputId": "bbd7102d-66fb-43b1-cfd0-e00574a76ce2",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: Jane Austen\n",
      "Text: “The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”\n",
      "Next page: /tag/humor/page/2/\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/tag/humor/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = scrapy.Selector(text=html)\n",
    "quote = sel.css(\"div.quote\")\n",
    "author = quote.xpath(\"span/small/text()\").get()\n",
    "print(\"Author:\", author)\n",
    "text = quote.css(\"span.text::text\").get()\n",
    "print(\"Text:\", text)\n",
    "next_page = sel.css(\"li.next a::attr(href)\").get()\n",
    "print(\"Next page:\", next_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lEgazWXOCJFQ",
    "outputId": "2683890a-032c-4068-cae9-0398a2745daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'tutorial', using template directory '/Work/venv/lib/python3.8/site-packages/scrapy/templates/project', created in:\n",
      "    /Work/Web_scraping/Tutorial/tutorial\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd tutorial\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf tutorial\n",
    "scrapy startproject tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AN4jZIGICJFV",
    "outputId": "a9da73a1-4f78-4449-836f-f7e1408c0d6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tutorial/quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tutorial/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/tag/humor/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css(\"div.quote\"):\n",
    "            yield {\n",
    "                \"author\": quote.xpath(\"span/small/text()\").get(),\n",
    "                \"text\": quote.css(\"span.text::text\").get()\n",
    "            }\n",
    "\n",
    "        next_page = response.css(\"li.next a::attr(href)\").get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lYj3sx7CJFY",
    "outputId": "fc28524c-96e9-480f-9aab-eede01ad8986",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 23:08:57 [scrapy.utils.log] INFO: Scrapy 2.10.1 started (bot: scrapybot)\n",
      "2023-09-10 23:08:57 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-6.2.0-31-generic-x86_64-with-glibc2.29\n",
      "2023-09-10 23:08:57 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-09-10 23:08:57 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'SPIDER_LOADER_WARN_ONLY': True}\n",
      "2023-09-10 23:08:57 [py.warnings] WARNING: /Work/venv/lib/python3.8/site-packages/scrapy/utils/request.py:248: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-09-10 23:08:57 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2023-09-10 23:08:57 [scrapy.extensions.telnet] INFO: Telnet Password: a18899a3a7b0ecaa\n",
      "2023-09-10 23:08:57 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-10 23:08:57 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-10 23:08:57 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-10 23:08:57 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-10 23:08:57 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-10 23:08:57 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-10 23:08:57 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-10 23:08:59 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/tag/humor/> (referer: None)\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Jane Austen', 'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Steve Martin', 'text': '“A day without sunshine is like, you know, night.”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Garrison Keillor', 'text': '“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Jim Henson', 'text': '“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Charles M. Schulz', 'text': \"“All you need is love. But a little chocolate now and then doesn't hurt.”\"}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Suzanne Collins', 'text': \"“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\"}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Charles Bukowski', 'text': '“Some people never go crazy. What truly horrible lives they must lead.”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Terry Pratchett', 'text': '“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'Dr. Seuss', 'text': '“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/>\n",
      "{'author': 'George Carlin', 'text': '“The reason I talk to myself is because I’m the only one whose answers I accept.”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/tag/humor/page/2/> (referer: https://quotes.toscrape.com/tag/humor/)\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/page/2/>\n",
      "{'author': 'W.C. Fields', 'text': '“I am free of all prejudice. I hate everyone equally. ”'}\n",
      "2023-09-10 23:09:00 [scrapy.core.scraper] DEBUG: Scraped from <200 https://quotes.toscrape.com/tag/humor/page/2/>\n",
      "{'author': 'Jane Austen', 'text': \"“A lady's imagination is very rapid; it jumps from admiration to love, from love to matrimony in a moment.”\"}\n",
      "2023-09-10 23:09:00 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-10 23:09:00 [scrapy.extensions.feedexport] INFO: Stored jl feed (12 items) in: ./tutorial/data/quotes.jl\n",
      "2023-09-10 23:09:00 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 528,\n",
      " 'downloader/request_count': 2,\n",
      " 'downloader/request_method_count/GET': 2,\n",
      " 'downloader/response_bytes': 15667,\n",
      " 'downloader/response_count': 2,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'elapsed_time_seconds': 2.66417,\n",
      " 'feedexport/success_count/FileFeedStorage': 1,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 10, 23, 9, 0, 647321),\n",
      " 'item_scraped_count': 12,\n",
      " 'log_count/DEBUG': 15,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 58277888,\n",
      " 'memusage/startup': 58277888,\n",
      " 'request_depth_max': 1,\n",
      " 'response_received_count': 2,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2023, 9, 10, 23, 8, 57, 983151)}\n",
      "2023-09-10 23:09:00 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "scrapy runspider ./tutorial/quotes_spider.py -O ./tutorial/data/quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIeH3gbpFNwQ",
    "outputId": "12c4a9ef-1052-4731-8090-7d0c27c8705c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./tutorial/tutorial/spiders/quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "import pathlib\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "\n",
    "    def start_requests(self):\n",
    "        urls = [\n",
    "            \"https://quotes.toscrape.com/page/1/\",\n",
    "            \"https://quotes.toscrape.com/page/2/\"\n",
    "        ]\n",
    "        for url in urls:\n",
    "            yield scrapy.Request(url=url, callback=self.parse)\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"./data/quotes-{page}.html\"\n",
    "        pathlib.Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "7Ck1_F6jFD53",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "655e1795-902a-4246-fc43-8218aebd00d6",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 23:09:52 [scrapy.utils.log] INFO: Scrapy 2.10.1 started (bot: tutorial)\n",
      "2023-09-10 23:09:52 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-6.2.0-31-generic-x86_64-with-glibc2.29\n",
      "2023-09-10 23:09:52 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-09-10 23:09:52 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'tutorial',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-09-10 23:09:52 [asyncio] DEBUG: Using selector: EpollSelector\n",
      "2023-09-10 23:09:52 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-09-10 23:09:52 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2023-09-10 23:09:52 [scrapy.extensions.telnet] INFO: Telnet Password: a5148ec1f16baeb3\n",
      "2023-09-10 23:09:52 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-10 23:09:52 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-10 23:09:52 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-10 23:09:52 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-10 23:09:52 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-10 23:09:52 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-10 23:09:52 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-10 23:09:53 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
      "2023-09-10 23:09:54 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
      "2023-09-10 23:09:54 [quotes] DEBUG: Saved file ./data/quotes-1.html\n",
      "2023-09-10 23:09:55 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
      "2023-09-10 23:09:55 [quotes] DEBUG: Saved file ./data/quotes-2.html\n",
      "2023-09-10 23:09:55 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-10 23:09:55 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 702,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 25556,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 2.630794,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 10, 23, 9, 55, 391870),\n",
      " 'log_count/DEBUG': 8,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 57880576,\n",
      " 'memusage/startup': 57880576,\n",
      " 'response_received_count': 3,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2023, 9, 10, 23, 9, 52, 761076)}\n",
      "2023-09-10 23:09:55 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ./tutorial\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./tutorial/tutorial/spiders/quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "import pathlib\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f\"./data/quotes-{page}.html\"\n",
    "        pathlib.Path(filename).write_bytes(response.body)\n",
    "        self.log(f\"Saved file {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 23:23:22 [scrapy.utils.log] INFO: Scrapy 2.10.1 started (bot: tutorial)\n",
      "2023-09-10 23:23:22 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-6.2.0-31-generic-x86_64-with-glibc2.29\n",
      "2023-09-10 23:23:22 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-09-10 23:23:22 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'BOT_NAME': 'tutorial',\n",
      " 'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'NEWSPIDER_MODULE': 'tutorial.spiders',\n",
      " 'REQUEST_FINGERPRINTER_IMPLEMENTATION': '2.7',\n",
      " 'ROBOTSTXT_OBEY': True,\n",
      " 'SPIDER_MODULES': ['tutorial.spiders'],\n",
      " 'TWISTED_REACTOR': 'twisted.internet.asyncioreactor.AsyncioSelectorReactor'}\n",
      "2023-09-10 23:23:22 [asyncio] DEBUG: Using selector: EpollSelector\n",
      "2023-09-10 23:23:22 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.asyncioreactor.AsyncioSelectorReactor\n",
      "2023-09-10 23:23:22 [scrapy.utils.log] DEBUG: Using asyncio event loop: asyncio.unix_events._UnixSelectorEventLoop\n",
      "2023-09-10 23:23:22 [scrapy.extensions.telnet] INFO: Telnet Password: 29ac14783d972c45\n",
      "2023-09-10 23:23:22 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2023-09-10 23:23:22 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.robotstxt.RobotsTxtMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-10 23:23:22 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-10 23:23:22 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-10 23:23:22 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-10 23:23:22 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2023-09-10 23:23:22 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-10 23:23:25 [scrapy.core.engine] DEBUG: Crawled (404) <GET https://quotes.toscrape.com/robots.txt> (referer: None)\n",
      "2023-09-10 23:23:25 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/1/> (referer: None)\n",
      "2023-09-10 23:23:25 [quotes] DEBUG: Saved file ./data/quotes-1.html\n",
      "2023-09-10 23:23:27 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com/page/2/> (referer: None)\n",
      "2023-09-10 23:23:27 [quotes] DEBUG: Saved file ./data/quotes-2.html\n",
      "2023-09-10 23:23:27 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2023-09-10 23:23:27 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 702,\n",
      " 'downloader/request_count': 3,\n",
      " 'downloader/request_method_count/GET': 3,\n",
      " 'downloader/response_bytes': 25556,\n",
      " 'downloader/response_count': 3,\n",
      " 'downloader/response_status_count/200': 2,\n",
      " 'downloader/response_status_count/404': 1,\n",
      " 'elapsed_time_seconds': 4.570821,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2023, 9, 10, 23, 23, 27, 241077),\n",
      " 'log_count/DEBUG': 8,\n",
      " 'log_count/INFO': 10,\n",
      " 'memusage/max': 57925632,\n",
      " 'memusage/startup': 57925632,\n",
      " 'response_received_count': 3,\n",
      " 'robotstxt/request_count': 1,\n",
      " 'robotstxt/response_count': 1,\n",
      " 'robotstxt/response_status_count/404': 1,\n",
      " 'scheduler/dequeued': 2,\n",
      " 'scheduler/dequeued/memory': 2,\n",
      " 'scheduler/enqueued': 2,\n",
      " 'scheduler/enqueued/memory': 2,\n",
      " 'start_time': datetime.datetime(2023, 9, 10, 23, 23, 22, 670256)}\n",
      "2023-09-10 23:23:27 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "cd ./tutorial\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Selector query='descendant-or-self::title' data='<title>Quotes to Scrape</title>'>]\n",
      "Quotes to Scrape\n",
      "['Quotes to Scrape']\n",
      "['Quotes']\n",
      "['Quotes', 'Scrape']\n",
      "[<Selector query='//title' data='<title>Quotes to Scrape</title>'>]\n",
      "Quotes to Scrape\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/page/1/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = scrapy.Selector(text=html)\n",
    "\n",
    "print(sel.css(\"title\"))\n",
    "print(sel.css(\"title::text\").get())\n",
    "print(sel.css(\"title::text\").re(r\"Quotes.*\"))\n",
    "print(sel.css(\"title::text\").re(r\"Q\\w+\"))\n",
    "print(sel.css(\"title::text\").re(r\"(\\w+) to (\\w+)\"))\n",
    "\n",
    "print(sel.xpath(\"//title\"))\n",
    "print(sel.xpath(\"//title/text()\").get())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 23:54:20 [scrapy.utils.log] INFO: Scrapy 2.10.1 started (bot: scrapybot)\n",
      "2023-09-10 23:54:20 [scrapy.utils.log] INFO: Versions: lxml 4.9.3.0, libxml2 2.10.3, cssselect 1.2.0, parsel 1.8.1, w3lib 2.1.2, Twisted 22.10.0, Python 3.8.10 (default, Mar 13 2023, 10:26:41) - [GCC 9.4.0], pyOpenSSL 23.2.0 (OpenSSL 3.1.2 1 Aug 2023), cryptography 41.0.3, Platform Linux-6.2.0-31-generic-x86_64-with-glibc2.29\n",
      "2023-09-10 23:54:20 [scrapy.addons] INFO: Enabled addons:\n",
      "[]\n",
      "2023-09-10 23:54:20 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'DUPEFILTER_CLASS': 'scrapy.dupefilters.BaseDupeFilter',\n",
      " 'LOGSTATS_INTERVAL': 0}\n",
      "2023-09-10 23:54:20 [py.warnings] WARNING: /Work/venv/lib/python3.8/site-packages/scrapy/utils/request.py:248: ScrapyDeprecationWarning: '2.6' is a deprecated value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting.\n",
      "\n",
      "It is also the default value. In other words, it is normal to get this warning if you have not defined a value for the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting. This is so for backward compatibility reasons, but it will change in a future version of Scrapy.\n",
      "\n",
      "See the documentation of the 'REQUEST_FINGERPRINTER_IMPLEMENTATION' setting for information on how to handle this deprecation.\n",
      "  return cls(crawler)\n",
      "\n",
      "2023-09-10 23:54:20 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2023-09-10 23:54:20 [scrapy.extensions.telnet] INFO: Telnet Password: 3dd7f2114abc3617\n",
      "2023-09-10 23:54:21 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage']\n",
      "2023-09-10 23:54:21 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2023-09-10 23:54:21 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2023-09-10 23:54:21 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2023-09-10 23:54:21 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2023-09-10 23:54:21 [scrapy.core.engine] INFO: Spider opened\n",
      "2023-09-10 23:54:23 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://quotes.toscrape.com> (referer: None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[s] Available Scrapy objects:\n",
      "[s]   scrapy     scrapy module (contains scrapy.Request, scrapy.Selector, etc)\n",
      "[s]   crawler    <scrapy.crawler.Crawler object at 0x7fbfd9247100>\n",
      "[s]   item       {}\n",
      "[s]   request    <GET https://quotes.toscrape.com>\n",
      "[s]   response   <200 https://quotes.toscrape.com>\n",
      "[s]   settings   <scrapy.settings.Settings object at 0x7fbfd92473d0>\n",
      "[s]   spider     <DefaultSpider 'default' at 0x7fbfd8cb6d60>\n",
      "[s] Useful shortcuts:\n",
      "[s]   fetch(url[, redirect=True]) Fetch URL and update local objects (by default, redirects are followed)\n",
      "[s]   fetch(req)                  Fetch a scrapy.Request and update local objects \n",
      "[s]   shelp()           Shell help (print this help)\n",
      "[s]   view(response)    View response in a browser\n",
      "In [1]: \n",
      "In [1]: \n",
      "In [2]: Out[2]: <Selector query=\"descendant-or-self::div[@class and contains(concat(' ', normalize-space(@class), ' '), ' quote ')]\" data='<div class=\"quote\" itemscope itemtype...'>\n",
      "\n",
      "In [3]: \n",
      "In [4]: Out[4]: '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”'\n",
      "\n",
      "In [5]: Do you really want to exit ([y]/n)? \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import scrapy\n",
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = scrapy.Selector(text=html)\n",
    "\n",
    "quote = response.css(\"div.quote\")[0]\n",
    "quote\n",
    "text = quote.css(\"span.text::text\").get()\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqcv_k7tCJFn",
    "outputId": "a8362a3d-e257-4fce-f39e-f2e69678eb3c"
   },
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/page/1/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "# CSS\n",
    "title = sel.css('title::text')\n",
    "print(f\"css title: {title}\")\n",
    "print(title.re(r'(\\w+) to (\\w+)'))\n",
    "\n",
    "# XPath\n",
    "title = sel.xpath('//title/text()')\n",
    "print(f\"xpath title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E2o-JD25CJFp",
    "outputId": "3df0327a-9d9d-4fb6-e168-b63d7f1e2517",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "\n",
    "for quote in sel.css('div.quote'):\n",
    "    text = quote.css('span.text::text').extract_first()\n",
    "    author = quote.css('small.author::text').extract_first()\n",
    "    tags = quote.css('div.tags a.tag::text').extract()\n",
    "    print(dict(text=text, author=author, tags=tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SnvXeOJt3KK",
    "outputId": "90a06139-1b97-45a6-fefc-4a1271211cf7"
   },
   "outputs": [],
   "source": [
    "%%writefile ./Tutorial/Tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "      'https://quotes.toscrape.com/page/1/',\n",
    "      'https://quotes.toscrape.com/page/2/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBmQ1PfP6SDC",
    "outputId": "35af10e9-bec4-4e9f-d85b-ab4f689819d5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0aFigT5eCJFr"
   },
   "source": [
    "### Storing the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roz26Udn6cDj",
    "outputId": "efd1e2ec-fdc1-43a0-f770-ba15cf448dc3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes -O ./data/quotes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok8vKftv7dDq",
    "outputId": "964b3726-9cf6-4ce4-c96e-51323207b78d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes -o ./data/quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kzp4uildCJFv"
   },
   "source": [
    "## Following links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tzhbQfBkCJFw",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "scrapy shell \"https://quotes.toscrape.com\"\n",
    "response.css(\"li.next a\").get()\n",
    "response.css(\"li.next a::attr(href)\").get()\n",
    "response.css(\"li.next a\").attrib[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gJ_J7N1CJFw"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "      'https://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "acmICjxSCJFx",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Moiw3bkNCJFy"
   },
   "source": [
    "### Supports relative URLs directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HSrsFY4pCJFz"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66jeaDK8CJFz",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CXeh8nZ2CJF0"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zz8tx-EqCJF0",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YQB5CAhDCJF1"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        for a in response.css('ul.pager li.next a'):\n",
    "            yield response.follow(a, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eKjNALacCJF1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9kHIFzPCJF2"
   },
   "source": [
    "### Create multiple requests from an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U6dqwRX-CJF3"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        anchors = response.css('ul.pager li.next a')\n",
    "        yield from response.follow_all(anchors, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EBY4_jlgCJF4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCTtEFJ4CJF5"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        yield from response.follow_all(css='ul.pager li.next a', callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eTDBh7ZFCJF5",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S1_jnBpXCJF5"
   },
   "source": [
    "## More patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I3ChZ_6SCJF5"
   },
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/author_spider.py\n",
    "import scrapy\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "\n",
    "    start_urls = ['https://quotes.toscrape.com/']\n",
    "\n",
    "    def parse(self, response):\n",
    "        author_page_links = response.css('.author + a')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "name": "Scrapy",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
