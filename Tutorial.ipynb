{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/MengOonLee/Web_scraping/blob/master/Tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No broken requirements found.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "pip install --no-cache-dir -qU pip wheel\n",
    "pip install --no-cache-dir -qU numpy pandas matplotlib seaborn\n",
    "pip install --no-cache-dir -qU scrapy\n",
    "pip check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Scrapy project 'Tutorial', using template directory '/home/meng/venv/lib/python3.8/site-packages/scrapy/templates/project', created in:\n",
      "    /home/meng/work/Web_scraping/Tutorial\n",
      "\n",
      "You can start your first spider with:\n",
      "    cd Tutorial\n",
      "    scrapy genspider example example.com\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# Create tutorial project\n",
    "scrapy startproject Tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./Tutorial/venv.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/venv.sh\n",
    "pip install --no-cache-dir -U pip wheel build\n",
    "pip install --no-cache-dir -U scrapy\n",
    "pip check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## XPaths & Selectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attributes\n",
    "\n",
    "`<tag attrib=\"attrib info\">...</tag>`\n",
    "  - `<div id=\"unique\" class=\"non unique\">...</div>`  \n",
    "  - `<a href=\"https://...\">...</a>`\n",
    "\n",
    "`@`: attributes\n",
    "  - `@id`, `@class`, `@href`\n",
    "  \n",
    "### XPaths notation\n",
    "\n",
    "xpath = '//*[@id=\"uid\"]/p[2]'  \n",
    "- `/`: look forward one generation  \n",
    "- `[]`: narrow on specific elements  \n",
    "- `//`: look forward all generations  \n",
    "- `*`: wildcard\n",
    "\n",
    "xpath = '//*[contains(@class, \"expr\")]'\n",
    "\n",
    "xpath = '//*/@class'\n",
    "\n",
    "XPath: `<xpath-to-element>/@attr-name`  \n",
    "xpath = '//div[@id=\"uid\"]/a/@href'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selector object\n",
    "# Import a scrapy Selector\n",
    "from scrapy import Selector\n",
    "\n",
    "# Import requests\n",
    "import requests\n",
    "url = \"https://en.wikipedia.org/wiki/Web_scraping\"\n",
    "# Create the string html containing the HTML source\n",
    "html = requests.get(url).content\n",
    "\n",
    "# Create the Selector object sel from html\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Outputs the SelectorList:\n",
    "sel.xpath('//p')\n",
    "# out: [<Selector xpath='//p' data='<p>..</p>'>, ...]\n",
    "\n",
    "sel.xpath('//p').extract()\n",
    "# out: ['<p>...</p>', ...]\n",
    "\n",
    "sel.xpath('//p').extract_first()\n",
    "# out: '<p>...</p>'\n",
    "\n",
    "# Text extraction for future generations\n",
    "sel.xpath('//p[@id=\"uid\"]//text()').extract()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSS & Response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS Locator\n",
    "\n",
    "CSS: Cascading Style Sheets  \n",
    "`/` replaced by `>`\n",
    "- XPath: `/html/body/div`  \n",
    "- CSS: `html > body > div`\n",
    "\n",
    "`//` replaced by ` `\n",
    "- XPath: `//div/span//p`  \n",
    "- CSS: `div > span p`\n",
    "\n",
    "`[N]` replaced by `:nth-of-type(N)`\n",
    "- XPath: `//div/p[2]`  \n",
    "- CSS: `div > p:nth-of-type(2)`\n",
    "\n",
    "`<tag>.<class>`: find element by class  \n",
    "`<tag>#<id>`: find element by id\n",
    "\n",
    "CSS Locator: `<css-to-element>::attr(attr-name)`  \n",
    "css_locator = 'div#uid > a::attr(href)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector from the html\n",
    "sel = Selector(text=html)\n",
    "\n",
    "sel.css('div > p')\n",
    "# out: [<Selector xpath='...' data='<p>...</p>'>, ...]\n",
    "\n",
    "sel.css('div > p').extract()\n",
    "# out: [<p>...</p>, ...]\n",
    "\n",
    "# hyperlink children of all div belongs to class course-block\n",
    "sel.css('div.course-block > a')\n",
    "\n",
    "# all element's class = class-1\n",
    "sel.css('.class-1')\n",
    "\n",
    "# Create the CSS Locator to all children of the element whose id is uid\n",
    "sel.css('#uid > *')\n",
    "\n",
    "# Text extraction for future generations\n",
    "sel.css('p#uid ::text').extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XPaths Notation & CSS Locators\n",
    "from scrapy import Selector\n",
    "\n",
    "# Create a selector object from a secret website\n",
    "sel = Selector(text=html)\n",
    "\n",
    "# Select all hyperlinks of div elements belonging to class \"course-block\"\n",
    "course_as = sel.css('div.course-block > a')\n",
    "\n",
    "# Selecting all href attributes chaining with css\n",
    "hrefs_from_css = course_as.css('::attr(href)')\n",
    "\n",
    "# Selecting all href attributes chaining with xpath\n",
    "hrefs_from_xpath = course_as.xpath('./@href')\n",
    "\n",
    "# Create an XPath string to the desired text.\n",
    "xpath = '//p[@id=\"p3\"]/text()'\n",
    "# Create a CSS Locator string to the desired text.\n",
    "css_locator = 'p#p3::text'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Response\n",
    "\n",
    "- has all the tools with Selectors  \n",
    "- keeps track of the url  \n",
    "- move from one side to another  \n",
    "\n",
    "XPath:  \n",
    "response.xpath('//div/span[@class=\"bio\"]')  \n",
    "CSS:  \n",
    "response.css('div > span.bio')  \n",
    "Chaining:  \n",
    "response.xpath('//div').css('span.bio')  \n",
    "\n",
    "`response.url`: keeps track URL  \n",
    "`response.follow(next_url)`: follow a new link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the URL to the website loaded in response\n",
    "this_url = response.url\n",
    "\n",
    "# Get the title of the website loaded in response\n",
    "this_title = response.xpath('/html/head/title')\\\n",
    "    .css('::text').extract_first()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "title: ['5 Reasons to Attend Extract Summit 2022', '5 Reasons to Attend Extract Summit 2022', 'How to use Playwright with Zyte Smart Proxy Manager', 'How to use Selenium with Zyte Smart Proxy Manager', 'How to use Puppeteer with Zyte Smart Proxy Manager', 'How to avoid web scraping blocks and bans', 'How web scraping is utilized for used car data extraction', 'Scraping large e-commerce websites: A guide for large scale scraping', 'A developer’s guide to rotating proxies in Python\\xa0', 'The importance of web scraping in data journalism', 'How can the travel industry benefit from data scraping?']\n",
      "next page: ['/blog/page/2/']\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "url = \"https://www.zyte.com/blog/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "title = sel.css('.oxy-post-title')\\\n",
    "    .css('::text').extract()\n",
    "print(f\"title: {title}\")\n",
    "\n",
    "next_page = sel.css('a.next')\\\n",
    "    .xpath('./@href').extract()\n",
    "print(f\"next page: {next_page}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./Tutorial/myspider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/myspider.py\n",
    "import scrapy\n",
    "\n",
    "class BlogSpider(scrapy.Spider):\n",
    "    name ='blogspider'\n",
    "    start_urls = [\"https://www.zyte.com/blog/\"]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for title in response.css('.oxy-post-title'):\n",
    "            yield {'title': title.css('::text').get()}\n",
    "            \n",
    "        for next_page in response.css('a.next'):\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./Tutorial/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy runspider myspider.py -O ./data/myspider.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "next page: ['/tag/humor/page/2/']\n",
      "author: ['Jane Austen', 'Steve Martin', 'Garrison Keillor', 'Jim Henson', 'Charles M. Schulz', 'Suzanne Collins', 'Charles Bukowski', 'Terry Pratchett', 'Dr. Seuss', 'George Carlin']\n",
      "text: ['“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', '“A day without sunshine is like, you know, night.”', '“Anyone who thinks sitting in church can make you a Christian must also think that sitting in a garage can make you a car.”', '“Beauty is in the eye of the beholder and it may be necessary from time to time to give a stupid or misinformed beholder a black eye.”', \"“All you need is love. But a little chocolate now and then doesn't hurt.”\", \"“Remember, we're madly in love, so it's all right to kiss me anytime you feel like it.”\", '“Some people never go crazy. What truly horrible lives they must lead.”', '“The trouble with having an open mind, of course, is that people will insist on coming along and trying to put things in it.”', '“Think left and think right and think low and think high. Oh, the thinks you can think up if only you try!”', '“The reason I talk to myself is because I’m the only one whose answers I accept.”']\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "url = \"https://quotes.toscrape.com/tag/humor/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "next_page = sel.css('li.next a::attr(\"href\")').extract()\n",
    "print(f\"next page: {next_page}\")\n",
    "\n",
    "quote = sel.css('div.quote')\n",
    "author = quote.xpath('span/small/text()').extract()\n",
    "print(f\"author: {author}\")\n",
    "text = quote.css('span.text::text').extract()\n",
    "print(f\"text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kDDBjF019o71",
    "outputId": "b879accb-647b-42ef-e5fb-f16c2310f99e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./Tutorial/quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = 'quotes'\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/tag/humor/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'author': quote.xpath('span/small/text()').get(),\n",
    "                'text': quote.css('span.text::text').get()\n",
    "            }\n",
    "\n",
    "        next_page = response.css('li.next a::attr(\"href\")').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wTGIArVO9tCh",
    "outputId": "0a78d94f-3bdb-45b3-d8a5-b575bf8e484c",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Tutorial/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy runspider quotes_spider.py -O ./data/quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NIeH3gbpFNwQ",
    "outputId": "12c4a9ef-1052-4731-8090-7d0c27c8705c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Tutorial/Tutorial/spiders/quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/Tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        \"https://quotes.toscrape.com/page/1/\",\n",
    "        \"https://quotes.toscrape.com/page/2/\"\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        page = response.url.split(\"/\")[-2]\n",
    "        filename = f'./data/quotes-{page}.html'\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ck1_F6jFD53",
    "outputId": "655e1795-902a-4246-fc43-8218aebd00d6",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Tutorial/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "css title: [<Selector xpath='descendant-or-self::title/text()' data='Quotes to Scrape'>]\n",
      "['Quotes', 'Scrape']\n",
      "xpath title: [<Selector xpath='//title/text()' data='Quotes to Scrape'>]\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/page/1/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "# CSS\n",
    "title = sel.css('title::text')\n",
    "print(f\"css title: {title}\")\n",
    "print(title.re(r'(\\w+) to (\\w+)'))\n",
    "\n",
    "# XPath\n",
    "title = sel.xpath('//title/text()')\n",
    "print(f\"xpath title: {title}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': '“The world as we have created it is a process of our thinking. It cannot be changed without changing our thinking.”', 'author': 'Albert Einstein', 'tags': ['change', 'deep-thoughts', 'thinking', 'world']}\n",
      "{'text': '“It is our choices, Harry, that show what we truly are, far more than our abilities.”', 'author': 'J.K. Rowling', 'tags': ['abilities', 'choices']}\n",
      "{'text': '“There are only two ways to live your life. One is as though nothing is a miracle. The other is as though everything is a miracle.”', 'author': 'Albert Einstein', 'tags': ['inspirational', 'life', 'live', 'miracle', 'miracles']}\n",
      "{'text': '“The person, be it gentleman or lady, who has not pleasure in a good novel, must be intolerably stupid.”', 'author': 'Jane Austen', 'tags': ['aliteracy', 'books', 'classic', 'humor']}\n",
      "{'text': \"“Imperfection is beauty, madness is genius and it's better to be absolutely ridiculous than absolutely boring.”\", 'author': 'Marilyn Monroe', 'tags': ['be-yourself', 'inspirational']}\n",
      "{'text': '“Try not to become a man of success. Rather become a man of value.”', 'author': 'Albert Einstein', 'tags': ['adulthood', 'success', 'value']}\n",
      "{'text': '“It is better to be hated for what you are than to be loved for what you are not.”', 'author': 'André Gide', 'tags': ['life', 'love']}\n",
      "{'text': \"“I have not failed. I've just found 10,000 ways that won't work.”\", 'author': 'Thomas A. Edison', 'tags': ['edison', 'failure', 'inspirational', 'paraphrased']}\n",
      "{'text': \"“A woman is like a tea bag; you never know how strong it is until it's in hot water.”\", 'author': 'Eleanor Roosevelt', 'tags': ['misattributed-eleanor-roosevelt']}\n",
      "{'text': '“A day without sunshine is like, you know, night.”', 'author': 'Steve Martin', 'tags': ['humor', 'obvious', 'simile']}\n"
     ]
    }
   ],
   "source": [
    "from scrapy import Selector\n",
    "import requests\n",
    "\n",
    "url = \"https://quotes.toscrape.com/\"\n",
    "html = requests.get(url).content\n",
    "\n",
    "sel = Selector(text=html)\n",
    "\n",
    "for quote in sel.css('div.quote'):\n",
    "    text = quote.css('span.text::text').extract_first()\n",
    "    author = quote.css('small.author::text').extract_first()\n",
    "    tags = quote.css('div.tags a.tag::text').extract()\n",
    "    print(dict(text=text, author=author, tags=tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SnvXeOJt3KK",
    "outputId": "90a06139-1b97-45a6-fefc-4a1271211cf7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Tutorial/Tutorial/spiders/quotes_spider.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/Tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "      'https://quotes.toscrape.com/page/1/',\n",
    "      'https://quotes.toscrape.com/page/2/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hBmQ1PfP6SDC",
    "outputId": "35af10e9-bec4-4e9f-d85b-ab4f689819d5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Tutorial/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing the scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "roz26Udn6cDj",
    "outputId": "efd1e2ec-fdc1-43a0-f770-ba15cf448dc3",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Tutorial/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes -O ./data/quotes.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ok8vKftv7dDq",
    "outputId": "964b3726-9cf6-4ce4-c96e-51323207b78d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./Tutorial/run.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./Tutorial/run.sh\n",
    "#!/bin/bash\n",
    "\n",
    "scrapy crawl quotes -o ./data/quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Following links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "scrapy shell \"https://quotes.toscrape.com\"\n",
    "response.css(\"li.next a\").get()\n",
    "response.css(\"li.next a::attr(href)\").get()\n",
    "response.css(\"li.next a\").attrib[\"href\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "      'https://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "\n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            next_page = response.urljoin(next_page)\n",
    "            yield scrapy.Request(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supports relative URLs directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com/page/1/'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        next_page = response.css('li.next a::attr(href)').get()\n",
    "        if next_page is not None:\n",
    "            yield response.follow(next_page, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        for href in response.css('li.next a::attr(href)'):\n",
    "            yield response.follow(href, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        for a in response.css('ul.pager li.next a'):\n",
    "            yield response.follow(a, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create multiple requests from an iterable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        anchors = response.css('ul.pager li.next a')\n",
    "        yield from response.follow_all(anchors, callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/quotes_spider.py\n",
    "import scrapy\n",
    "\n",
    "class QuotesSpider(scrapy.Spider):\n",
    "    name = \"quotes\"\n",
    "    start_urls = [\n",
    "        'https://quotes.toscrape.com'\n",
    "    ]\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for quote in response.css('div.quote'):\n",
    "            yield {\n",
    "                'text': quote.css('span.text::text').get(),\n",
    "                'author': quote.css('small.author::text').get(),\n",
    "                'tags': quote.css('div.tags a.tag::text').getall()\n",
    "            }\n",
    "        yield from response.follow_all(css='ul.pager li.next a', callback=self.parse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd tutorial\n",
    "rm -rf tutorial/quotes.jl\n",
    "scrapy crawl quotes -o quotes.jl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./tutorial/tutorial/spiders/author_spider.py\n",
    "import scrapy\n",
    "\n",
    "class AuthorSpider(scrapy.Spider):\n",
    "    name = 'author'\n",
    "    \n",
    "    start_urls = ['https://quotes.toscrape.com/']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        author_page_links = response.css('.author + a')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyOJeDMq173KlrdlGG8ohN66",
   "include_colab_link": true,
   "name": "Tutorial",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
